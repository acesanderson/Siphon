gemma3:27b
llama4:16x17b
qwen2.5vl:32b
llava:13b (34b?)
llama3.2-vision:11b
minicpm-v:8b
granite3.2-vision:2b


gemma3
The current, most capable model that runs on a single GPU.
1b
4b
12b
27b

llama4
Meta's latest collection of multimodal models.
tools
16x17b
128x17b

qwen2.5vl
Flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.
3b
7b
32b
72b

llava
ðŸŒ‹ LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.
7b
13b
34b

llama3.2-vision
Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.
11b
90b

minicpm-v
A series of multimodal LLMs (MLLMs) designed for vision-language understanding.
8b

llava-llama3
A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.
8b

granite3.2-vision
A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more.
2b
